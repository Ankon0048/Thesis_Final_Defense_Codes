{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Starting of Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and PIP installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-18T18:43:57.605567Z",
     "iopub.status.busy": "2025-06-18T18:43:57.605201Z",
     "iopub.status.idle": "2025-06-18T18:43:58.699973Z",
     "shell.execute_reply": "2025-06-18T18:43:58.698794Z",
     "shell.execute_reply.started": "2025-06-18T18:43:57.605532Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from scipy.signal import find_peaks, savgol_filter\n",
    "from scipy.ndimage import binary_closing\n",
    "import secrets      # cryptographically‑secure RNG\n",
    "import base64       # for compact ASCII/“number + letter” output\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-18T18:46:16.073573Z",
     "iopub.status.busy": "2025-06-18T18:46:16.072723Z",
     "iopub.status.idle": "2025-06-18T18:46:16.080606Z",
     "shell.execute_reply": "2025-06-18T18:46:16.079252Z",
     "shell.execute_reply.started": "2025-06-18T18:46:16.073520Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Input image\n",
    "image_path = \"D://Thesis//Final_Thesis_Pipeline//kaggle//input//tests//a01-132x.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"kaggle/input/weights/last.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-18T18:46:28.313114Z",
     "iopub.status.busy": "2025-06-18T18:46:28.312614Z",
     "iopub.status.idle": "2025-06-18T18:46:28.322862Z",
     "shell.execute_reply": "2025-06-18T18:46:28.321607Z",
     "shell.execute_reply.started": "2025-06-18T18:46:28.313087Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def seperate_handwritten_printed_using_yolo(image_path, folder_name):\n",
    "    results = model([image_path])  \n",
    "    image = cv2.imread(image_path)\n",
    "    base_name = os.path.basename(image_path)\n",
    "\n",
    "    base_crop_folder = \"cropped_outputs\"\n",
    "    base_graph_folder = \"graph_outputs\"\n",
    "\n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        cls = boxes.cls.cpu().numpy()\n",
    "        xyxy = boxes.xyxy.cpu().numpy()\n",
    "\n",
    "        class9_boxes = [box for i, box in enumerate(xyxy) if cls[i] == 9]\n",
    "\n",
    "        def ensure_and_save_crop(box, root_folder):\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            cropped = image[y1:y2, x1:x2]\n",
    "            subfolder = os.path.join(folder_name, root_folder)\n",
    "            os.makedirs(subfolder, exist_ok=True)\n",
    "            filename = f\"{uuid.uuid4().hex}.jpg\"\n",
    "            print(cv2.imwrite(os.path.join(subfolder, filename), cropped))\n",
    "\n",
    "        # Save all class 9 cropped regions\n",
    "        for box in class9_boxes:\n",
    "            ensure_and_save_crop(box, base_crop_folder)\n",
    "\n",
    "        # Save image with detection graph\n",
    "        graph = result.plot()\n",
    "        graph_folder = os.path.join(folder_name, base_graph_folder)\n",
    "        os.makedirs(graph_folder, exist_ok=True)\n",
    "        print(cv2.imwrite(os.path.join(graph_folder, base_name), graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-18T18:46:30.611963Z",
     "iopub.status.busy": "2025-06-18T18:46:30.611608Z",
     "iopub.status.idle": "2025-06-18T18:46:36.780968Z",
     "shell.execute_reply": "2025-06-18T18:46:36.779948Z",
     "shell.execute_reply.started": "2025-06-18T18:46:30.611937Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x448 2 Page-headers, 1 Section-header, 2 Texts, 86.6ms\n",
      "Speed: 5.0ms preprocess, 86.6ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "seperate_handwritten_printed_using_yolo(image_path, \"kaggle/output/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting lines from the detected image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-18T18:53:01.267462Z",
     "iopub.status.busy": "2025-06-18T18:53:01.266162Z",
     "iopub.status.idle": "2025-06-18T18:53:01.272664Z",
     "shell.execute_reply": "2025-06-18T18:53:01.271629Z",
     "shell.execute_reply.started": "2025-06-18T18:53:01.267300Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Update your base directories\n",
    "base_input_dir = r\"kaggle/output/cropped_outputs\"\n",
    "base_output_printed = r\"kaggle/output/cropped_outputs_line\"\n",
    "base_graph_folder = r\"kaggle/output/line_graphs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-18T18:53:03.039578Z",
     "iopub.status.busy": "2025-06-18T18:53:03.039226Z",
     "iopub.status.idle": "2025-06-18T18:53:03.063803Z",
     "shell.execute_reply": "2025-06-18T18:53:03.062914Z",
     "shell.execute_reply.started": "2025-06-18T18:53:03.039555Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from scipy.signal import find_peaks, savgol_filter\n",
    "from scipy.ndimage import binary_closing\n",
    "\n",
    "def auto_savgol_smooth(profile, polyorder=2, spacing_factor=None,\n",
    "                       plot=True, plot_title=\"\", save_path=None,\n",
    "                       show_thresholds=False, high_thresh=None, low_thresh=None):\n",
    "    \n",
    "    peaks, _ = find_peaks(profile, distance=8)\n",
    "    if len(peaks) < 2:\n",
    "        raise ValueError(\"Not enough peaks detected to estimate line spacing.\")\n",
    "\n",
    "    diffs = np.diff(peaks)\n",
    "    eps = 1e-9\n",
    "    weights = 1.0 / (diffs + eps)\n",
    "    avg_spacing = int(np.round(np.sum(weights * diffs) / np.sum(weights)))\n",
    "\n",
    "    # Dynamically estimate spacing_factor if not provided\n",
    "    if spacing_factor is None:\n",
    "        spacing_factor = min(max(1.2, avg_spacing / 20), 2.0)\n",
    "\n",
    "    window_length = int(spacing_factor * avg_spacing)\n",
    "    if window_length % 2 == 0:\n",
    "        window_length += 1\n",
    "    window_length = max(window_length, polyorder + 4)\n",
    "    window_length = min(window_length,\n",
    "                        len(profile) - 1 if len(profile) % 2 else len(profile) - 2)\n",
    "\n",
    "    smoothed = savgol_filter(profile, window_length=window_length, polyorder=polyorder)\n",
    "\n",
    "    if plot:\n",
    "        fig = plt.figure(figsize=(14, 5))\n",
    "        plt.plot(profile, label=\"Original\", color=\"orange\", alpha=0.6)\n",
    "        plt.plot(smoothed, label=f\"Smoothed (window={window_length})\", color=\"blue\")\n",
    "        plt.plot(peaks, profile[peaks], \"rx\", label=\"Detected Peaks\")\n",
    "\n",
    "        if show_thresholds:\n",
    "            if high_thresh is not None:\n",
    "                plt.axhline(y=high_thresh, color=\"red\", linestyle=\"--\", label=f\"High Thresh = {high_thresh:.2f}\")\n",
    "            if low_thresh is not None:\n",
    "                plt.axhline(y=low_thresh, color=\"green\", linestyle=\"--\", label=f\"Low Thresh = {low_thresh:.2f}\")\n",
    "\n",
    "        plt.title(plot_title or \"Savitzky-Golay smoothing\")\n",
    "        plt.xlabel(\"Row Index\")\n",
    "        plt.ylabel(\"Sum of Pixel Intensities\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        if save_path is not None:\n",
    "            fig.savefig(save_path)\n",
    "        plt.close(fig)\n",
    "\n",
    "    return smoothed, spacing_factor\n",
    "\n",
    "\n",
    "\n",
    "def calculate_projection_profile_and_crop_lines_with_lines(image_path, folder_name):\n",
    "    base_name = os.path.basename(image_path)\n",
    "    image_name_no_ext = os.path.splitext(base_name)[0]\n",
    "\n",
    "    subfolder_graph = os.path.join(base_graph_folder, folder_name)\n",
    "    os.makedirs(subfolder_graph, exist_ok=True)\n",
    "    output_path = os.path.join(subfolder_graph, f\"{base_name}\")\n",
    "\n",
    "    image = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image {image_path}\")\n",
    "        return\n",
    "\n",
    "    _, binary_image = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    horizontal_projection = np.sum(binary_image, axis=1)\n",
    "\n",
    "    smoothed, spacing_factor = auto_savgol_smooth(\n",
    "        horizontal_projection,\n",
    "        save_path=output_path,\n",
    "        plot=True,\n",
    "        show_thresholds=True\n",
    "    )\n",
    "\n",
    "    # === Dynamic Thresholds ===\n",
    "    Q1 = np.percentile(smoothed, 25)\n",
    "    Q3 = np.percentile(smoothed, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    mean_val = np.mean(smoothed)\n",
    "    min_val = np.min(smoothed)\n",
    "    max_val = np.max(smoothed)\n",
    "\n",
    "    iqr_low = Q1 + 0.2 * IQR\n",
    "    iqr_high = iqr_low + 0.2 * IQR\n",
    "    mean_low = mean_val * 0.25\n",
    "    mean_high = mean_val * 0.5\n",
    "    scaled_low = min_val + 0.1 * (max_val - min_val)\n",
    "    scaled_high = min_val + 0.3 * (max_val - min_val)\n",
    "\n",
    "    low_thresh = np.median([iqr_low, mean_low, scaled_low])\n",
    "    high_thresh = np.median([iqr_high, mean_high, scaled_high])\n",
    "\n",
    "    # Re-plot with thresholds\n",
    "    smoothed, _ = auto_savgol_smooth(\n",
    "        horizontal_projection,\n",
    "        spacing_factor=spacing_factor,\n",
    "        save_path=output_path,\n",
    "        plot=True,\n",
    "        show_thresholds=True,\n",
    "        high_thresh=high_thresh,\n",
    "        low_thresh=low_thresh\n",
    "    )\n",
    "\n",
    "    # === Line Detection with Relaxed High Threshold at Bottom ===\n",
    "    line_ranges = []\n",
    "    is_in_line = False\n",
    "    relaxed_zone = int(0.8 * len(smoothed))\n",
    "\n",
    "    for row, value in enumerate(smoothed):\n",
    "        current_high = high_thresh\n",
    "        if row > relaxed_zone:\n",
    "            current_high = high_thresh * 0.65  # relax threshold in bottom zone\n",
    "\n",
    "        if value > current_high and not is_in_line:\n",
    "            start_row = row\n",
    "            is_in_line = True\n",
    "        elif value < low_thresh and is_in_line:\n",
    "            end_row = row\n",
    "            line_ranges.append((start_row, end_row))\n",
    "            is_in_line = False\n",
    "\n",
    "    if is_in_line:\n",
    "        line_ranges.append((start_row, len(smoothed)))\n",
    "\n",
    "    # === Fallback: Recover Missed Final Line ===\n",
    "    last_line_margin = int(len(smoothed) * 0.17)\n",
    "    end_threshold = len(smoothed) - last_line_margin\n",
    "    last_part_vals = smoothed[-last_line_margin:]\n",
    "\n",
    "    if all(end < end_threshold for _, end in line_ranges):\n",
    "        if np.max(last_part_vals) > low_thresh:\n",
    "            fallback_start = end_threshold\n",
    "            line_ranges.append((fallback_start, len(smoothed)))\n",
    "\n",
    "    # === Refine Borders ===\n",
    "    if line_ranges:\n",
    "        line_ranges[0] = (max(0, line_ranges[0][0] - 5), line_ranges[0][1])\n",
    "        line_ranges[-1] = (line_ranges[-1][0], min(image.shape[0], line_ranges[-1][1] + 5))\n",
    "\n",
    "    for i in range(1, len(line_ranges)):\n",
    "        temp = (line_ranges[i - 1][1] + line_ranges[i][0]) // 2\n",
    "        line_ranges[i - 1] = (line_ranges[i - 1][0], temp)\n",
    "        line_ranges[i] = (temp, line_ranges[i][1])\n",
    "\n",
    "    line_ranges = sorted(line_ranges, key=lambda x: x[0])\n",
    "\n",
    "    # === Save Cropped Lines ===\n",
    "    subfolder_output = os.path.join(base_output_printed, folder_name, image_name_no_ext)\n",
    "    os.makedirs(subfolder_output, exist_ok=True)\n",
    "\n",
    "    for idx, (start, end) in enumerate(line_ranges, 1):\n",
    "        cropped_line = image[start:end, :]\n",
    "        save_path = os.path.join(subfolder_output, f\"{idx}.png\")\n",
    "        print(f'save_path = {save_path}')\n",
    "        cv2.imwrite(save_path, cropped_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-18T18:53:08.238688Z",
     "iopub.status.busy": "2025-06-18T18:53:08.238265Z",
     "iopub.status.idle": "2025-06-18T18:53:08.245716Z",
     "shell.execute_reply": "2025-06-18T18:53:08.244438Z",
     "shell.execute_reply.started": "2025-06-18T18:53:08.238662Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === Batch processor ===\n",
    "def process_all_images():\n",
    "    for root, dirs, files in os.walk(base_input_dir):\n",
    "        image_files = sorted(\n",
    "            [f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))]\n",
    "        )\n",
    "        for file in image_files:\n",
    "            image_path = os.path.join(root, file)\n",
    "            folder_name = os.path.basename(root)\n",
    "            try:\n",
    "                calculate_projection_profile_and_crop_lines_with_lines(image_path, folder_name)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {image_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-18T18:53:09.632277Z",
     "iopub.status.busy": "2025-06-18T18:53:09.631912Z",
     "iopub.status.idle": "2025-06-18T18:53:10.138098Z",
     "shell.execute_reply": "2025-06-18T18:53:10.137252Z",
     "shell.execute_reply.started": "2025-06-18T18:53:09.632248Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_path = kaggle/output/cropped_outputs_line\\cropped_outputs\\0dffb5d295124f7983a3ded7dbb8ab96\\1.png\n",
      "save_path = kaggle/output/cropped_outputs_line\\cropped_outputs\\0dffb5d295124f7983a3ded7dbb8ab96\\2.png\n",
      "save_path = kaggle/output/cropped_outputs_line\\cropped_outputs\\0dffb5d295124f7983a3ded7dbb8ab96\\3.png\n"
     ]
    }
   ],
   "source": [
    "process_all_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting whether Image is handwritten or printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "# Constants\n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the same model class\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.fc = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Create model and load weights\n",
    "model = SimpleCNN().to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"kaggle/input/weights/final_model_weights_HvP.pth\", map_location=DEVICE))\n",
    "# model.eval()\n",
    "\n",
    "# Test transforms\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-18T17:55:53.128025Z",
     "iopub.status.busy": "2025-06-18T17:55:53.127395Z",
     "iopub.status.idle": "2025-06-18T17:55:53.138114Z",
     "shell.execute_reply": "2025-06-18T17:55:53.136599Z",
     "shell.execute_reply.started": "2025-06-18T17:55:53.127992Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# --- Load and preprocess the image ---\n",
    "def predict_HvP(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_tensor = test_transform(image).unsqueeze(0).to(DEVICE)  # Add batch dimension\n",
    "    \n",
    "    # --- Make prediction ---\n",
    "    # model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        predicted_class = output.argmax(1).item()\n",
    "    \n",
    "    # --- Map class index to class name ---\n",
    "    class_names = [\"handwritten\", \"printed\"]  # Get class names from dataset\n",
    "    print(f\"Predicted class label: {class_names[predicted_class]}\")\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting text from image using TrOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-18T17:56:11.425455Z",
     "iopub.status.busy": "2025-06-18T17:56:11.424393Z",
     "iopub.status.idle": "2025-06-18T17:56:11.433136Z",
     "shell.execute_reply": "2025-06-18T17:56:11.431961Z",
     "shell.execute_reply.started": "2025-06-18T17:56:11.425423Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-large-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "import os\n",
    "\n",
    "# Load models and processors only once\n",
    "hand_written_model_id = \"microsoft/trocr-large-handwritten\"\n",
    "printed_model_id = \"microsoft/trocr-base-printed\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on {device}\")\n",
    "\n",
    "# Load both models and processors once\n",
    "printed_processor = TrOCRProcessor.from_pretrained(printed_model_id)\n",
    "printed_model = VisionEncoderDecoderModel.from_pretrained(printed_model_id).to(device)\n",
    "\n",
    "handwritten_processor = TrOCRProcessor.from_pretrained(hand_written_model_id)\n",
    "handwritten_model = VisionEncoderDecoderModel.from_pretrained(hand_written_model_id).to(device)\n",
    "\n",
    "# OCR runner\n",
    "def run_trOCR(model, processor, image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    generated_ids = model.generate(pixel_values, max_new_tokens=1000)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    # print(f\"{os.path.basename(image_path)} -> {generated_text}\")\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Prediction for the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import torch\n",
    "\n",
    "# Load the small grammar correction model (T5)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"vennify/t5-base-grammar-correction\")\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"vennify/t5-base-grammar-correction\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_case_and_punctuation(text: str, max_length=128):\n",
    "    input_text = \"grammar: \" + text.lower()\n",
    "    input_ids = t5_tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "    outputs = t5_model.generate(input_ids, max_length=max_length, num_beams=4, early_stopping=True)\n",
    "    corrected = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-18T18:10:45.729089Z",
     "iopub.status.busy": "2025-06-18T18:10:45.728734Z",
     "iopub.status.idle": "2025-06-18T18:10:45.736317Z",
     "shell.execute_reply": "2025-06-18T18:10:45.735220Z",
     "shell.execute_reply.started": "2025-06-18T18:10:45.729062Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class label: handwritten\n",
      "Fixed Text: I love Kuet.\n",
      "Predicted class label: handwritten\n",
      "Fixed Text: It is a wonderful thing.\n"
     ]
    }
   ],
   "source": [
    "# Path to input images\n",
    "folder_dir = \"kaggle/output/cropped_outputs_line/cropped_outputs/\"\n",
    "for paths in os.listdir(folder_dir):\n",
    "    \n",
    "    input_dir = os.path.join(folder_dir,paths)       \n",
    "    # Collect image file paths\n",
    "    file_paths = [\n",
    "        os.path.join(input_dir, filename)\n",
    "        for filename in (os.listdir(input_dir))\n",
    "    ]\n",
    "\n",
    "    # Loop through and run OCR\n",
    "    for path in file_paths:\n",
    "        # img = cv2.imread(path)\n",
    "        # plt.imshow(img, cmap='gray')\n",
    "        raw_text = \"\"\n",
    "        if predict_HvP(path) == 1:\n",
    "            raw_text = ((run_trOCR(printed_model, printed_processor, path)))\n",
    "        else:\n",
    "            raw_text = ((run_trOCR(handwritten_model, handwritten_processor, path)))\n",
    "        \n",
    "        fixed_text = restore_case_and_punctuation(raw_text)\n",
    "        print(\"Fixed Text:\", fixed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03337cf371ad40f091731d8324ac69cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/406 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ArT\\.cache\\huggingface\\hub\\models--oliverguhr--fullstop-punctuation-multilang-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a426dd4130d34ac5b26071d6ec4408b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/892 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24eae54682964240b3d6f66fe1dd4ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a56f72ffb546abbf35b55107b230a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e140e34644d9477b9b89d615aed63ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4696ac215fd745768389db88f6005475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a good spelling system it work we\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "from symspellpy import SymSpell\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "# ---------------------------\n",
    "# Setup Spell Correction (SymSpell)\n",
    "# ---------------------------\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\"\n",
    ")\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "# ---------------------------\n",
    "# Setup Punctuation Restoration\n",
    "# ---------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"oliverguhr/fullstop-punctuation-multilang-large\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"oliverguhr/fullstop-punctuation-multilang-large\")\n",
    "punct_pipeline = pipeline(\"token-classification\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# ---------------------------\n",
    "# Main Function\n",
    "# ---------------------------\n",
    "def restore_text(text: str) -> str:\n",
    "    # Step 1: Spell Correction\n",
    "    suggestions = sym_spell.lookup_compound(text, max_edit_distance=2)\n",
    "    corrected_text = suggestions[0].term if suggestions else text\n",
    "    \n",
    "    # Step 2: Case Restoration (simple heuristic)\n",
    "    corrected_text = corrected_text.strip()\n",
    "    if corrected_text:  \n",
    "        corrected_text = corrected_text[0].upper() + corrected_text[1:]\n",
    "    corrected_text = corrected_text.replace(\" i \", \" I \")\n",
    "    \n",
    "    # Step 3: Punctuation Restoration\n",
    "    tokens = corrected_text.split()\n",
    "    predictions = punct_pipeline(corrected_text)\n",
    "    \n",
    "    restored_text = \"\"\n",
    "    for i, token in enumerate(tokens):\n",
    "        restored_text += token\n",
    "        # Add punctuation if predicted\n",
    "        if i < len(predictions) and predictions[i]['word'] in [\".\", \",\", \"?\", \"!\"]:\n",
    "            restored_text += predictions[i]['word']\n",
    "        restored_text += \" \"\n",
    "    \n",
    "    return restored_text.strip()\n",
    "\n",
    "# ---------------------------\n",
    "# Example Usage\n",
    "# --------------------------\n",
    "print(restore_text(\"i havv a goood speling systm it work well\"))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7683776,
     "sourceId": 12198063,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7683938,
     "sourceId": 12209503,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7691437,
     "sourceId": 12209516,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
